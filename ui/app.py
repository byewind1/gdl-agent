"""
gdl-agent Web UI â€” Streamlit interface for architects.

Run: streamlit run ui/app.py
"""

import sys
import re
import os
import time
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import streamlit as st

from gdl_agent.hsf_project import HSFProject, ScriptType, GDLParameter
from gdl_agent.gdl_parser import parse_gdl_source, parse_gdl_file
from gdl_agent.paramlist_builder import build_paramlist_xml, validate_paramlist
from gdl_agent.compiler import MockHSFCompiler, HSFCompiler, CompileResult
from gdl_agent.core import GDLAgent, Status
from gdl_agent.knowledge import KnowledgeBase
from gdl_agent.skills_loader import SkillsLoader


# â”€â”€ Page Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="gdl-agent",
    page_icon="ğŸ—ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Custom CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Noto+Sans+SC:wght@300;400;600&display=swap');

.stApp { font-family: 'Noto Sans SC', sans-serif; }
code, .stCodeBlock { font-family: 'JetBrains Mono', monospace !important; }

.main-header {
    font-family: 'JetBrains Mono', monospace;
    font-size: 2rem; font-weight: 600;
    background: linear-gradient(135deg, #22d3ee, #34d399);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    margin-bottom: 0;
}
.sub-header { color: #94a3b8; font-size: 0.9rem; margin-top: -0.5rem; margin-bottom: 2rem; }

.welcome-card {
    background: linear-gradient(135deg, #0f172a, #1e293b);
    border: 1px solid #334155;
    border-radius: 12px;
    padding: 2rem;
    margin: 1rem 0;
}
.step-item {
    display: flex;
    align-items: flex-start;
    gap: 0.75rem;
    margin-bottom: 1rem;
    padding: 0.75rem;
    background: #1e293b;
    border-radius: 8px;
    border-left: 3px solid #22d3ee;
}
</style>
""", unsafe_allow_html=True)


# â”€â”€ Session State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if "project" not in st.session_state:
    st.session_state.project = None
if "compile_log" not in st.session_state:
    st.session_state.compile_log = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "work_dir" not in st.session_state:
    st.session_state.work_dir = str(Path.home() / "gdl-agent-workspace")
if "agent_running" not in st.session_state:
    st.session_state.agent_running = False
if "model_api_keys" not in st.session_state:
    # Per-model API Key storage â€” pre-fill from config.toml provider_keys
    st.session_state.model_api_keys = {}


# â”€â”€ Load config.toml defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_config_defaults = {}
_provider_keys: dict = {}   # {provider: api_key}

try:
    from gdl_agent.config import GDLAgentConfig
    import sys as _sys, os as _os
    # Load raw TOML to get provider_keys nested table
    if _sys.version_info >= (3, 11):
        import tomllib as _tomllib
    else:
        import tomli as _tomllib   # type: ignore

    _toml_path = _os.path.join(_os.path.dirname(__file__), "..", "config.toml")
    if _os.path.exists(_toml_path):
        with open(_toml_path, "rb") as _f:
            _raw = _tomllib.load(_f)
        _provider_keys = _raw.get("llm", {}).get("provider_keys", {})

    _config = GDLAgentConfig.load()
    _config_defaults = {
        "llm_model": _config.llm.model,
        "compiler_path": _config.compiler.path or "",
    }
except Exception:
    pass


def _key_for_model(model: str) -> str:
    """Pick the right API Key from provider_keys based on model name."""
    m = model.lower()
    if "glm" in m:
        return _provider_keys.get("zhipu", "")
    elif "deepseek" in m and "ollama" not in m:
        return _provider_keys.get("deepseek", "")
    elif "claude" in m:
        return _provider_keys.get("anthropic", "")
    elif "gpt" in m or "o3" in m or "o1" in m:
        return _provider_keys.get("openai", "")
    elif "gemini" in m:
        return _provider_keys.get("google", "")
    return ""

# â”€â”€ Sidebar Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.markdown('<p class="main-header">gdl-agent</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">v0.4.1 Â· HSF-native Â· AI-powered</p>', unsafe_allow_html=True)
    st.divider()

    st.subheader("ğŸ“ å·¥ä½œç›®å½•")
    work_dir = st.text_input("Work Directory", value=st.session_state.work_dir, label_visibility="collapsed")
    st.session_state.work_dir = work_dir

    st.divider()
    st.subheader("ğŸ”§ ç¼–è¯‘å™¨ / Compiler")

    compiler_mode = st.radio(
        "ç¼–è¯‘æ¨¡å¼",
        ["Mock (æ— éœ€ ArchiCAD)", "LP_XMLConverter (çœŸå®ç¼–è¯‘)"],
        index=1 if _config_defaults.get("compiler_path") else 0,
    )

    converter_path = ""
    if compiler_mode.startswith("LP"):
        converter_path = st.text_input(
            "LP_XMLConverter è·¯å¾„",
            value=_config_defaults.get("compiler_path", ""),
            placeholder="/Applications/GRAPHISOFT/ArchiCAD 28/LP_XMLConverter",
        )

    st.divider()
    st.subheader("ğŸ§  AI æ¨¡å‹ / LLM")

    model_options = [
        # â”€â”€ Anthropic Claude â”€â”€
        "claude-haiku-4-5-20251001",
        "claude-sonnet-4-5-20250929",
        "claude-opus-4-5-20250918",
        "claude-opus-4-6",
        # â”€â”€ æ™ºè°± GLM (Z.ai) â”€â”€
        "glm-4.7",
        "glm-4.7-flash",
        "glm-4-plus",
        "glm-4-flash",
        # â”€â”€ OpenAI â”€â”€
        "gpt-4o",
        "gpt-4o-mini",
        "o3-mini",
        # â”€â”€ DeepSeek â”€â”€
        "deepseek-chat",
        "deepseek-reasoner",
        # â”€â”€ Google Gemini â”€â”€
        "gemini/gemini-2.5-flash",
        "gemini/gemini-2.5-pro",
        # â”€â”€ Ollama æœ¬åœ° â”€â”€
        "ollama/qwen2.5:14b",
        "ollama/qwen3:8b",
        "ollama/deepseek-coder-v2:16b",
    ]

    default_model = _config_defaults.get("llm_model", "glm-4.7")
    default_index = model_options.index(default_model) if default_model in model_options else 4

    model_name = st.selectbox("æ¨¡å‹ / Model", model_options, index=default_index)

    # Load or initialize API Key for this specific model
    if model_name not in st.session_state.model_api_keys:
        # Auto-fill from config.toml provider_keys
        st.session_state.model_api_keys[model_name] = _key_for_model(model_name)

    api_key = st.text_input(
        "API Key",
        value=st.session_state.model_api_keys.get(model_name, ""),
        type="password",
        help="Ollama æœ¬åœ°æ¨¡å¼ä¸éœ€è¦ Key"
    )

    # Auto-save API Key if user manually edited it
    if api_key != st.session_state.model_api_keys.get(model_name, ""):
        st.session_state.model_api_keys[model_name] = api_key

    if "claude" in model_name:
        st.caption("ğŸ”‘ [è·å– Claude API Key â†’](https://console.anthropic.com/settings/keys)")
        st.caption("âš ï¸ API Key éœ€å•ç‹¬å……å€¼ï¼Œä¸ Claude Pro è®¢é˜…é¢åº¦æ— å…³")
    elif "glm" in model_name:
        st.caption("ğŸ”‘ [è·å–æ™ºè°± API Key â†’](https://bigmodel.cn/usercenter/apikeys)")
    elif "gpt" in model_name or "o3" in model_name:
        st.caption("ğŸ”‘ [è·å– OpenAI API Key â†’](https://platform.openai.com/api-keys)")
    elif "deepseek" in model_name and "ollama" not in model_name:
        st.caption("ğŸ”‘ [è·å– DeepSeek API Key â†’](https://platform.deepseek.com/api_keys)")
    elif "gemini" in model_name:
        st.caption("ğŸ”‘ [è·å– Gemini API Key â†’](https://aistudio.google.com/apikey)")
    elif "ollama" in model_name:
        st.caption("ğŸ–¥ï¸ æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ Keyã€‚ç¡®ä¿ Ollama å·²å¯åŠ¨ã€‚")

    # API Base URL â€” only needed for OpenAI-compatible custom endpoints
    # zai/ (GLM), deepseek/, anthropic/ are native LiteLLM providers, no api_base needed
    def _get_default_api_base(model: str) -> str:
        m = model.lower()
        if "ollama" in m:
            return "http://localhost:11434"
        # GLM uses zai/ native provider â€” no api_base
        # DeepSeek uses deepseek/ native provider â€” no api_base
        return ""

    default_api_base = _get_default_api_base(model_name)
    api_base = ""
    if default_api_base:
        api_base = st.text_input("API Base URL", value=default_api_base)

    max_retries = st.slider("æœ€å¤§é‡è¯•æ¬¡æ•°", 1, 10, 5)

    st.divider()

    # Project info + quick reset
    if st.session_state.project:
        proj = st.session_state.project
        st.subheader(f"ğŸ“¦ {proj.name}")
        st.caption(f"å‚æ•°: {len(proj.parameters)} | è„šæœ¬: {len(proj.scripts)}")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤é¡¹ç›®", use_container_width=True):
            st.session_state.project = None
            st.session_state.chat_history = []
            st.rerun()


# â”€â”€ Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_compiler():
    if compiler_mode.startswith("Mock"):
        return MockHSFCompiler()
    return HSFCompiler(converter_path or None)

def get_llm():
    from gdl_agent.config import LLMConfig
    from gdl_agent.llm import LLMAdapter
    config = LLMConfig(
        model=model_name,
        api_key=api_key,
        api_base=api_base,
        temperature=0.2,
        max_tokens=4096,
    )
    return LLMAdapter(config)

def load_knowledge(task_type: str = "all"):
    kb_dir = Path(st.session_state.work_dir) / "knowledge"
    if not kb_dir.exists():
        kb_dir = Path(__file__).parent.parent / "knowledge"
    kb = KnowledgeBase(str(kb_dir))
    kb.load()
    return kb.get_by_task_type(task_type)

def load_skills():
    sk_dir = Path(st.session_state.work_dir) / "skills"
    if not sk_dir.exists():
        sk_dir = Path(__file__).parent.parent / "skills"
    sl = SkillsLoader(str(sk_dir))
    sl.load()
    return sl

def _versioned_gsm_path(proj_name: str, work_dir: str) -> str:
    """
    Return next available versioned GSM path.
    MyShelf_v1.gsm â†’ MyShelf_v2.gsm â†’ ...
    Preserves all previous compilations.
    """
    out_dir = Path(work_dir) / "output"
    out_dir.mkdir(parents=True, exist_ok=True)
    v = 1
    while (out_dir / f"{proj_name}_v{v}.gsm").exists():
        v += 1
    return str(out_dir / f"{proj_name}_v{v}.gsm")


def _extract_project_name_regex(text: str) -> str:
    """Regex fallback: only use when LLM is unavailable."""
    patterns = [
        r'named?\s+([A-Za-z][A-Za-z0-9_]{2,30})',
        r'called\s+([A-Za-z][A-Za-z0-9_]{2,30})',
        r'åä¸º\s*([A-Za-z][A-Za-z0-9_]{2,30})',
        r'å«\s*([A-Za-z][A-Za-z0-9_]{2,30})',
    ]
    for pat in patterns:
        m = re.search(pat, text, re.IGNORECASE)
        if m:
            return m.group(1)
    return "MyObject"


def _extract_project_name(text: str, llm=None) -> str:
    """
    Extract a valid GDL object name from user description.
    Uses LLM for Chinese/ambiguous input; falls back to regex.
    """
    if llm is not None:
        try:
            resp = llm.generate([
                {
                    "role": "system",
                    "content": (
                        "Extract a short English GDL library object name from the user description. "
                        "Rules: CamelCase, letters and digits only, 3-24 chars. "
                        "Good examples: Bookshelf, WallPanel, WindowFrame, DoorUnit, StairStep, ColumnBase. "
                        "Reply with ONLY the name â€” no explanation, no quotes."
                    ),
                },
                {"role": "user", "content": text},
            ], max_tokens=12, temperature=0)
            name = resp.content.strip().split()[0]  # take first word only
            if re.match(r'^[A-Za-z][A-Za-z0-9]{2,23}$', name):
                return name
        except Exception:
            pass
    return _extract_project_name_regex(text)


# â”€â”€ Welcome / Onboarding Panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def show_welcome():
    st.markdown("""
<div class="welcome-card">
<h2 style="color:#22d3ee; margin-top:0; font-family:'JetBrains Mono';">æ¬¢è¿ä½¿ç”¨ gdl-agent ğŸ—ï¸</h2>
<p style="color:#94a3b8;">ç”¨è‡ªç„¶è¯­è¨€é©±åŠ¨ ArchiCAD GDL å¯¹è±¡çš„åˆ›å»ºä¸ç¼–è¯‘ã€‚æ— éœ€äº†è§£ GDL è¯­æ³•ï¼Œç›´æ¥æè¿°éœ€æ±‚å³å¯ã€‚</p>
</div>
""", unsafe_allow_html=True)

    st.markdown("#### ä¸‰æ­¥å¿«é€Ÿå¼€å§‹")

    st.info("**â‘  é…ç½® API Key**  \nåœ¨å·¦ä¾§è¾¹æ é€‰æ‹© AI æ¨¡å‹ï¼Œå¡«å…¥å¯¹åº” API Keyã€‚å…è´¹çš„æ™ºè°± GLM å¯ç›´æ¥ä½¿ç”¨ã€‚")
    st.info("**â‘¡ å¼€å§‹å¯¹è¯**  \nåœ¨åº•éƒ¨è¾“å…¥æ¡†æè¿°ä½ æƒ³åˆ›å»ºçš„ GDL å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š  \nã€Œåˆ›å»ºä¸€ä¸ªå®½ 600mmã€æ·± 400mm çš„ä¹¦æ¶ï¼Œå¸¦ iShelves å‚æ•°æ§åˆ¶å±‚æ•°ã€")
    st.info("**â‘¢ ç¼–è¯‘è¾“å‡º**  \nAI ç”Ÿæˆä»£ç åè‡ªåŠ¨è§¦å‘ç¼–è¯‘ã€‚çœŸå®ç¼–è¯‘éœ€åœ¨ä¾§è¾¹æ é…ç½® LP_XMLConverter è·¯å¾„ã€‚Mock æ¨¡å¼å¯éªŒè¯ç»“æ„ï¼Œæ— éœ€å®‰è£… ArchiCADã€‚")

    st.divider()

    st.markdown("#### æˆ–è€…ï¼šå¯¼å…¥å·²æœ‰ GDL æ–‡ä»¶")
    uploaded_file = st.file_uploader(
        "æ‹–å…¥ .gdl æ–‡ä»¶å¼€å§‹ç¼–è¾‘",
        type=["gdl", "txt"],
        help="æ”¯æŒ AI ç”Ÿæˆæˆ–æ‰‹å†™çš„ GDL æºç ",
        key="welcome_upload",
    )
    if uploaded_file:
        content = uploaded_file.read().decode("utf-8", errors="replace")
        name = Path(uploaded_file.name).stem
        try:
            project = parse_gdl_source(content, name)
            project.work_dir = Path(st.session_state.work_dir)
            project.root = project.work_dir / project.name
            st.session_state.project = project
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": f"âœ… å·²å¯¼å…¥ `{project.name}` â€” {len(project.parameters)} ä¸ªå‚æ•°ï¼Œ{len(project.scripts)} ä¸ªè„šæœ¬ã€‚å¯ä»¥å¼€å§‹å¯¹è¯ä¿®æ”¹äº†ã€‚"
            })
            st.rerun()
        except Exception as e:
            st.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

    st.divider()
    st.caption("ğŸ’¡ æç¤ºï¼šç¬¬ä¸€æ¡æ¶ˆæ¯æ— éœ€åˆ›å»ºé¡¹ç›®ï¼Œç›´æ¥æè¿°éœ€æ±‚ï¼ŒAI ä¼šè‡ªåŠ¨åˆå§‹åŒ–ã€‚")


# â”€â”€ Intent Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_GDL_KEYWORDS = [
    "åˆ›å»º", "ç”Ÿæˆ", "åˆ¶ä½œ", "åšä¸€ä¸ª", "å»ºä¸€ä¸ª", "å†™ä¸€ä¸ª",
    "ä¿®æ”¹", "æ›´æ–°", "æ·»åŠ ", "åˆ é™¤", "è°ƒæ•´", "ä¼˜åŒ–",
    "ä¹¦æ¶", "æŸœå­", "çª—", "é—¨", "å¢™", "æ¥¼æ¢¯", "æ¡Œ", "æ¤…",
    "å‚æ•°", "parameter", "script", "gdl", "gsm", "hsf",
    "compile", "ç¼–è¯‘", "build", "create", "make", "add",
    "3d", "2d", "prism", "block", "sphere",
]

def _is_gdl_intent(text: str) -> bool:
    """Quick keyword check â€” if obvious GDL request, skip LLM classification."""
    t = text.lower()
    return any(kw in t for kw in _GDL_KEYWORDS)

def classify_intent(text: str, llm) -> str:
    """
    Return 'GDL' if user wants to create/modify GDL objects,
    otherwise 'CHAT' for casual conversation.
    """
    # Fast path: obvious keywords
    if _is_gdl_intent(text):
        return "GDL"

    # LLM-based classification for ambiguous cases
    try:
        resp = llm.generate([
            {
                "role": "system",
                "content": (
                    "You are an intent classifier for a GDL object builder app. "
                    "Reply with exactly one word: GDL or CHAT.\n"
                    "GDL = user wants to create, modify, compile, or ask technical questions about ArchiCAD GDL library objects.\n"
                    "CHAT = greeting, small talk, general questions about the app, or anything unrelated to GDL code generation."
                ),
            },
            {"role": "user", "content": text},
        ], max_tokens=5, temperature=0)
        result = resp.content.strip().upper()
        return "GDL" if "GDL" in result else "CHAT"
    except Exception:
        # If classification fails, default to CHAT (safer)
        return "CHAT"


def chat_respond(user_input: str, history: list, llm) -> str:
    """Simple conversational response without triggering Agent."""
    system_msg = {
        "role": "system",
        "content": (
            "ä½ æ˜¯ gdl-agent çš„åŠ©æ‰‹ï¼Œä¸“æ³¨äº ArchiCAD GDL åº“æ„ä»¶çš„åˆ›å»ºä¸ç¼–è¯‘ã€‚"
            "ç”¨æˆ·å¯ä»¥å’Œä½ é—²èŠï¼Œä¹Ÿå¯ä»¥è®©ä½ åˆ›å»º GDL å¯¹è±¡ã€‚"
            "é—²èŠæ—¶è‡ªç„¶å›åº”ï¼Œç®€æ´å‹å¥½ï¼›æ¶‰åŠ GDL åˆ›å»ºéœ€æ±‚æ—¶æé†’ç”¨æˆ·ç›´æ¥æè¿°éœ€æ±‚å³å¯å¼€å§‹ç”Ÿæˆã€‚"
            "å›å¤ä½¿ç”¨ä¸­æ–‡ï¼Œä¸“ä¸šæœ¯è¯­ä¿ç•™è‹±æ–‡ï¼ˆGDLã€HSFã€ArchiCADã€paramlist ç­‰ï¼‰ã€‚"
        ),
    }
    messages = [system_msg]
    # Include recent history for context (last 6 messages)
    for msg in history[-6:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": user_input})

    try:
        resp = llm.generate(messages)
        return resp.content
    except Exception as e:
        return f"âŒ {str(e)}"


# â”€â”€ Run Agent (shared logic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_agent(user_input: str, proj: HSFProject, status_col):
    """Run agent and return response message string."""
    events = []

    status_ph = status_col.empty()
    detail_ph = status_col.empty()

    def on_event(event_type, data):
        events.append((event_type, data))
        if event_type == "analyze":
            scripts = data.get("affected_scripts", [])
            status_ph.info(f"ğŸ” åˆ†æä¸­... å½±å“è„šæœ¬: {', '.join(scripts)}")
        elif event_type == "attempt":
            status_ph.info(f"ğŸ§  ç¬¬ {data['attempt']} æ¬¡å°è¯•ï¼Œè°ƒç”¨ AI...")
        elif event_type == "compile_start":
            status_ph.info("ğŸ”§ ç¼–è¯‘ä¸­...")
        elif event_type == "compile_error":
            detail_ph.warning(f"âš ï¸ ç¬¬ {data['attempt']} æ¬¡ç¼–è¯‘å¤±è´¥: {data['error'][:200]}")
        elif event_type == "success":
            status_ph.success(f"âœ… æˆåŠŸï¼ç¬¬ {data['attempt']} æ¬¡å°è¯•ç¼–è¯‘é€šè¿‡ã€‚")

    try:
        llm = get_llm()
        compiler = get_compiler()
        knowledge = load_knowledge()
        skills_loader = load_skills()
        skills_text = skills_loader.get_for_task(user_input)

        output_gsm = _versioned_gsm_path(proj.name, st.session_state.work_dir)

        agent = GDLAgent(
            llm=llm,
            compiler=compiler,
            max_iterations=max_retries,
            on_event=on_event,
        )

        result = agent.run(
            instruction=user_input,
            project=proj,
            output_gsm=output_gsm,
            knowledge=knowledge,
            skills=skills_text,
        )

        mock_tag = " [Mock]" if compiler_mode.startswith("Mock") else ""
        if result.status == Status.SUCCESS:
            msg = (
                f"âœ… **ç¼–è¯‘æˆåŠŸ{mock_tag}** â€” ç¬¬ {result.attempts} æ¬¡å°è¯•\n\n"
                f"ğŸ“¦ è¾“å‡º: `{result.output_path}`\n\n"
                f"å‚æ•°: {len(proj.parameters)} | "
                f"è„šæœ¬: {', '.join(st_type.value for st_type in proj.scripts)}"
            )
            if compiler_mode.startswith("Mock"):
                msg += "\n\nâš ï¸ Mock æ¨¡å¼ä¸ç”ŸæˆçœŸå® .gsmï¼Œåˆ‡æ¢åˆ° LP_XMLConverter è¿›è¡ŒçœŸå®ç¼–è¯‘ã€‚"
        elif result.status == Status.FAILED:
            msg = f"âŒ **å¤±è´¥**: {result.error_summary}"
        elif result.status == Status.EXHAUSTED:
            msg = (
                f"âš ï¸ **{result.attempts} æ¬¡å°è¯•åä»æœªæˆåŠŸ**\n\n"
                f"æœ€åé”™è¯¯: {result.error_summary[:300]}\n\n"
                f"å»ºè®®: æ¢ä¸€ç§æè¿°æ–¹å¼ï¼Œæˆ–åˆ‡æ¢åˆ°ã€Œç¼–è¾‘ã€Tab æ‰‹åŠ¨ä¿®æ”¹ä»£ç ã€‚"
            )
        else:
            msg = f"â›” ä»»åŠ¡è¢«é˜»æ­¢: {result.error_summary}"

        status_ph.empty()
        detail_ph.empty()

        st.session_state.compile_log.append({
            "project": proj.name,
            "instruction": user_input,
            "success": result.status == Status.SUCCESS,
            "attempts": result.attempts,
            "message": result.error_summary or "Success",
        })

        return msg

    except Exception as e:
        status_ph.empty()
        detail_ph.empty()
        return f"âŒ **é”™è¯¯**: {str(e)}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Main Layout: Left Chat | Right Editor
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

col_chat, col_editor = st.columns([2, 3], gap="large")


# â”€â”€ Left: Chat History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with col_chat:
    if not st.session_state.project:
        st.markdown("### ğŸ’¬ å¼€å§‹åˆ›å»º")
        st.markdown(
            '<p style="color:#64748b; font-size:0.9rem;">åœ¨åº•éƒ¨è¾“å…¥æ¡†æè¿°ä½ æƒ³åˆ›å»ºçš„å¯¹è±¡ï¼ŒAI ä¼šè‡ªåŠ¨ç”Ÿæˆå¹¶ç¼–è¯‘ã€‚</p>',
            unsafe_allow_html=True,
        )
    else:
        proj_now = st.session_state.project
        st.markdown(f"### ğŸ’¬ {proj_now.name}")
        st.caption(f"å‚æ•°: {len(proj_now.parameters)} | è„šæœ¬: {len(proj_now.scripts)}")

    # Chat history
    for msg in st.session_state.chat_history:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # Placeholder for live agent output (populated when agent runs)
    live_output = st.empty()


# â”€â”€ Right: Welcome / Project Editor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with col_editor:
    if not st.session_state.project:
        show_welcome()
    else:
        proj_now = st.session_state.project

        tab_edit, tab_compile, tab_log = st.tabs(["ğŸ“ ç¼–è¾‘", "ğŸ”§ ç¼–è¯‘", "ğŸ“‹ æ—¥å¿—"])

        # â”€â”€ Edit Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_edit:
            st.markdown("#### å‚æ•°åˆ—è¡¨")
            param_data = [
                {
                    "Type": p.type_tag,
                    "Name": p.name,
                    "Value": p.value,
                    "Description": p.description,
                    "Fixed": "âœ“" if p.is_fixed else "",
                }
                for p in proj_now.parameters
            ]
            if param_data:
                st.dataframe(param_data, use_container_width=True, hide_index=True)
            else:
                st.caption("æš‚æ— å‚æ•°ï¼Œé€šè¿‡å¯¹è¯è®© AI æ·»åŠ ï¼Œæˆ–æ‰‹åŠ¨æ·»åŠ ã€‚")

            with st.expander("â• æ‰‹åŠ¨æ·»åŠ å‚æ•°"):
                pc1, pc2, pc3, pc4 = st.columns(4)
                with pc1:
                    p_type = st.selectbox("Type", [
                        "Length", "Integer", "Boolean", "RealNum", "Angle",
                        "String", "Material", "FillPattern", "LineType", "PenColor",
                    ])
                with pc2:
                    p_name = st.text_input("Name", value="bNewParam")
                with pc3:
                    p_value = st.text_input("Value", value="0")
                with pc4:
                    p_desc = st.text_input("Description")
                if st.button("æ·»åŠ å‚æ•°"):
                    try:
                        proj_now.add_parameter(GDLParameter(p_name, p_type, p_desc, p_value))
                        st.success(f"âœ… {p_type} {p_name}")
                        st.rerun()
                    except Exception as e:
                        st.error(str(e))

            st.divider()
            st.markdown("#### è„šæœ¬")
            script_tabs = st.tabs(["3D", "2D", "Master", "Param", "UI", "Properties"])
            script_map = [
                (ScriptType.SCRIPT_3D, "3d.gdl"),
                (ScriptType.SCRIPT_2D, "2d.gdl"),
                (ScriptType.MASTER, "1d.gdl"),
                (ScriptType.PARAM, "vl.gdl"),
                (ScriptType.UI, "ui.gdl"),
                (ScriptType.PROPERTIES, "pr.gdl"),
            ]
            for tab, (stype, fname) in zip(script_tabs, script_map):
                with tab:
                    current = proj_now.get_script(stype)
                    new_content = st.text_area(
                        fname, value=current, height=280, key=f"script_{fname}",
                    )
                    if new_content != current:
                        proj_now.set_script(stype, new_content)

            if st.button("ğŸ” éªŒè¯å‚æ•°"):
                issues = validate_paramlist(proj_now.parameters)
                if issues:
                    for i in issues:
                        st.warning(i)
                else:
                    st.success("âœ… å‚æ•°éªŒè¯é€šè¿‡")

        # â”€â”€ Compile Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_compile:
            # Auto versioned output path â€” no manual input needed
            next_gsm = _versioned_gsm_path(proj_now.name, st.session_state.work_dir)
            next_ver = Path(next_gsm).stem  # e.g. MyShelf_v2
            st.caption(f"ğŸ“¦ ä¸‹æ¬¡ç¼–è¯‘è¾“å‡º: `{Path(next_gsm).name}`")

            col_c, col_p = st.columns([1, 1])

            with col_c:
                if st.button("ğŸ”§ æ‰‹åŠ¨ç¼–è¯‘", type="primary"):
                    output_path = _versioned_gsm_path(proj_now.name, st.session_state.work_dir)

                    with st.spinner("å†™å…¥ HSF..."):
                        try:
                            hsf_dir = proj_now.save_to_disk()
                        except Exception as e:
                            st.error(f"å†™å…¥å¤±è´¥: {e}")
                            st.stop()

                    with st.spinner("ç¼–è¯‘ä¸­..."):
                        compiler = get_compiler()
                        result = compiler.hsf2libpart(str(hsf_dir), output_path)

                    if result.success:
                        if compiler_mode.startswith("Mock"):
                            st.success(
                                f"âœ… **[Mock]** ç»“æ„éªŒè¯é€šè¿‡ï¼\n\n"
                                f"ğŸ“ HSF ç›®å½•: `{hsf_dir}`"
                            )
                        else:
                            st.success(f"âœ… ç¼–è¯‘æˆåŠŸï¼\n\nğŸ“¦ `{output_path}`")
                    else:
                        st.error(f"âŒ ç¼–è¯‘å¤±è´¥\n\n```\n{result.stderr}\n```")

                    st.session_state.compile_log.append({
                        "project": proj_now.name,
                        "instruction": "(manual compile)",
                        "success": result.success,
                        "attempts": 1,
                        "message": result.stderr or "Success",
                    })

            with col_p:
                st.markdown("##### é¢„è§ˆ")
                with st.expander("paramlist.xml"):
                    st.code(build_paramlist_xml(proj_now.parameters), language="xml")
                with st.expander("HSF ç›®å½•ç»“æ„", expanded=True):
                    tree = [f"ğŸ“ {proj_now.name}/", "  â”œâ”€â”€ libpartdata.xml",
                            "  â”œâ”€â”€ paramlist.xml", "  â”œâ”€â”€ ancestry.xml", "  â””â”€â”€ scripts/"]
                    for stype in ScriptType:
                        if stype in proj_now.scripts:
                            n = proj_now.scripts[stype].count("\n") + 1
                            tree.append(f"       â”œâ”€â”€ {stype.value} ({n} lines)")
                    st.code("\n".join(tree), language="text")

        # â”€â”€ Log Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_log:
            if not st.session_state.compile_log:
                st.info("æš‚æ— è®°å½•")
            else:
                for entry in reversed(st.session_state.compile_log):
                    icon = "âœ…" if entry["success"] else "âŒ"
                    instr = entry.get("instruction", "")
                    st.markdown(f"**{icon} {entry['project']}** â€” {instr}")
                    if entry.get("attempts", 0) > 1:
                        st.caption(f"å°è¯• {entry['attempts']} æ¬¡")
                    st.code(entry["message"], language="text")
                    st.divider()

            if st.button("æ¸…é™¤æ—¥å¿—"):
                st.session_state.compile_log = []
                st.rerun()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Chat Input â€” Always at Bottom
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

user_input = st.chat_input(
    "æè¿°ä½ æƒ³åˆ›å»ºæˆ–ä¿®æ”¹çš„ GDL å¯¹è±¡ï¼Œå¦‚ã€Œåˆ›å»ºä¸€ä¸ªå®½ 600mm çš„ä¹¦æ¶ï¼ŒiShelves æ§åˆ¶å±‚æ•°ã€"
)

if user_input:
    # Add user message to history
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    # Check API key first
    if not api_key and "ollama" not in model_name:
        err = "âŒ è¯·åœ¨å·¦ä¾§è¾¹æ å¡«å…¥ API Key åå†è¯•ã€‚"
        st.session_state.chat_history.append({"role": "assistant", "content": err})
        st.rerun()
    else:
        llm_for_classify = get_llm()

        # â”€â”€ Intent classification â”€â”€
        intent = classify_intent(user_input, llm_for_classify)

        with live_output.container():
            st.chat_message("user").markdown(user_input)
            with st.chat_message("assistant"):
                if intent == "CHAT":
                    # â”€â”€ Casual conversation â€” no project creation, no agent â”€â”€
                    msg = chat_respond(
                        user_input,
                        st.session_state.chat_history[:-1],  # exclude the just-added user msg
                        llm_for_classify,
                    )
                    st.markdown(msg)

                else:
                    # â”€â”€ GDL intent â€” create project if needed, then run agent â”€â”€
                    if not st.session_state.project:
                        proj_name = _extract_project_name(user_input, llm=llm_for_classify)
                        new_proj = HSFProject.create_new(proj_name, work_dir=st.session_state.work_dir)
                        st.session_state.project = new_proj
                        st.info(f"ğŸ“ å·²åˆå§‹åŒ–é¡¹ç›® `{proj_name}`")

                    proj_current = st.session_state.project
                    msg = run_agent(user_input, proj_current, st.container())
                    st.markdown(msg)

        st.session_state.chat_history.append({"role": "assistant", "content": msg})
        st.rerun()


# â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.divider()
st.markdown(
    '<p style="text-align:center; color:#64748b; font-size:0.8rem;">'
    'gdl-agent v0.4.1 Â· HSF-native Â· '
    '<a href="https://github.com/byewind/gdl-agent">GitHub</a>'
    '</p>',
    unsafe_allow_html=True,
)
